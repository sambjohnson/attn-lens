# attn-lens

How do small transformer models learn algorithmic tasks? In particular, how do they utilize their attention heads (or not) to rearrange information? This small set of experiments applies interpretive techniques to such toy models. It is part self-tutorial, part proof-of-concept, and very much in progress.
